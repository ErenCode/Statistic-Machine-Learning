{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "  \n",
    "  \n",
    "def judge_legal_url(one_str):  \n",
    "    ''''' \n",
    "    正则匹配方法 \n",
    "    判断一个字符串是否是合法IP地址 \n",
    "    '''  \n",
    "    compile_ip=re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')    \n",
    "    if compile_ip.match(one_str):    \n",
    "        return True    \n",
    "    else:    \n",
    "        return False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to watch Grey's on the big screen - Thursday indulgence..... \n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "file=open('train_tweets.txt','r',encoding=\"UTF-8\")#read the file\n",
    "predict_file=open('test_tweets_unlabeled.txt','r',encoding=\"UTF-8\")#read the predicted file\n",
    "y=[]# store the id\n",
    "x=[]#store the train tweet\n",
    "predictx=[]#store the predicted tweet\n",
    "\n",
    "for content in file.readlines(): # read all lines\n",
    "    content=content.rstrip(\"\\n\")    #remove the \\n\n",
    "    y.append(content.split(\"\\t\")[0])\n",
    "    x.append(content.split(\"\\t\")[1])\n",
    "    \n",
    "\n",
    "#preprocess the train_tweet\n",
    "train_x=[]\n",
    "for content in x:\n",
    "    string=\"\"\n",
    "    len_count=0\n",
    "    smile_count=0\n",
    "    #shenglvehao_count=0\n",
    "    for word in content.split(\" \"):\n",
    "        \n",
    "        \n",
    "        if judge_legal_url(word):\n",
    "            word=urlparse(word).netloc\n",
    "            stringU=\"\"\n",
    "            for i in word:\n",
    "                if i==\".\":\n",
    "                    continue\n",
    "                else:\n",
    "                    stringU+=i\n",
    "            word=stringU\n",
    "        elif word[0]=='#': \n",
    "            word=word[1:]+'tag'\n",
    "            \n",
    "       \n",
    "        \"\"\"\n",
    "        if \"..\" in word:\n",
    "            n=0\n",
    "            for i in word:\n",
    "                if i=='.':\n",
    "                    n+=1\n",
    "            word=word+' shenglvehao'+ str(n)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if \"!\" in word:\n",
    "            n=0\n",
    "            for i in word:\n",
    "                if i=='!':\n",
    "                    n+=1\n",
    "            word=word+' ghao'+ str(n)\n",
    "        \"\"\"\n",
    "        \"\"\"   \n",
    "        if \"?\" in word:\n",
    "            n=0\n",
    "            for i in word:\n",
    "                if i=='?':\n",
    "                    n+=1\n",
    "            word=word+' wenhao'+ str(n)\n",
    "            \n",
    "        if \":)\" in word:\n",
    "            word=word+' xlian'\n",
    "          \n",
    "        if \"~\" in word:\n",
    "            n=0\n",
    "            for i in word:\n",
    "                if i=='~':\n",
    "                    n+=1\n",
    "            word=word+' weiyin'+ str(n) \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        for i in range(len(word)-1):\n",
    "            if (word[i]==\":\" or word[i]==\";\") and word[i+1]==\")\":\n",
    "                smile_count+=1\n",
    "        \"\"\"    \n",
    "        #transfer all the word in lower case\n",
    "        string+=word+\" \"\n",
    "    \"\"\"    \n",
    "    if len_count >0:\n",
    "            string+=' '+'nolongword'+str(len_count)\n",
    "    \"\"\"\n",
    "    if smile_count>0:\n",
    "        string+=' xlian'+str(smile_count)       \n",
    "    train_x.append(string)  \n",
    "        \n",
    "#print(train_x[6])\n",
    "  \n",
    "for row in predict_file.readlines():\n",
    "    row=row.rstrip(\"\\n\")\n",
    "    predictx.append(row)\n",
    "\n",
    "#preprocess the predict_tweet\n",
    "predict_x=[]\n",
    "for content in predictx:\n",
    "    string=\"\"\n",
    "    len_count=0\n",
    "    smile_count=0\n",
    "    for word in content.split(\" \"):\n",
    "                \n",
    "            \n",
    "        if judge_legal_url(word):\n",
    "            word=urlparse(word).netloc\n",
    "            stringU=\"\"\n",
    "            for i in word:\n",
    "                if i==\".\":\n",
    "                    continue\n",
    "                else:\n",
    "                    stringU+=i\n",
    "            word=stringU\n",
    "        elif word[0]=='#':\n",
    "            word=word[1:]+'tag'\n",
    "\n",
    "        \"\"\"\n",
    "        if \"..\" in word:\n",
    "            n=0\n",
    "            for i in word:\n",
    "                if i=='.':\n",
    "                    n+=1\n",
    "            word=word+' shenglvehao'+ str(n)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if \"!\" in word:\n",
    "            n=0\n",
    "            for i in word:\n",
    "                if i=='!':\n",
    "                    n+=1\n",
    "            word=word+' ghao'+ str(n)\n",
    "        \"\"\"\n",
    "        \"\"\"   \n",
    "        if \"?\" in word:\n",
    "            n=0\n",
    "            for i in word:\n",
    "                if i=='?':\n",
    "                    n+=1\n",
    "            word=word+' wenhao'+ str(n)\n",
    "            \n",
    "        if \":)\" in word:\n",
    "            word=word+' xlian'\n",
    "          \n",
    "        if \"~\" in word:\n",
    "            n=0\n",
    "            for i in word:\n",
    "                if i=='~':\n",
    "                    n+=1\n",
    "            word=word+' weiyin'+ str(n)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        for i in range(len(word)-1):\n",
    "            if (word[i]==\":\" or word[i]==\";\") and word[i+1]==\")\":\n",
    "                smile_count+=1\n",
    "        \"\"\"\n",
    "        #transfer all the word in lower case\n",
    "        string+=word+\" \"\n",
    "       \n",
    "    if smile_count>0:\n",
    "        string+=' xlian'+str(smile_count) \n",
    "    predict_x.append(string) \n",
    "#print(predict_x[0:7])\n",
    "\n",
    "#print(train_x[6])\n",
    "#print(y)\n",
    "#print(predict_x[0])\n",
    "#print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @handle: Mickey Mouse is Getting a Make Over bitly @handle \n"
     ]
    }
   ],
   "source": [
    "print(train_x[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141104\n"
     ]
    }
   ],
   "source": [
    "#tf-idf preprocess\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    " \n",
    " \n",
    "corpus = train_x\n",
    " \n",
    "vectorizer=CountVectorizer(analyzer='word',stop_words='english')\n",
    " \n",
    "transformer = TfidfTransformer()\n",
    "tfidf_x = transformer.fit_transform(vectorizer.fit_transform(corpus)) \n",
    "\n",
    "tfidf_predict_x=vectorizer.transform(predict_x)\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "print (len(feature_name))\n",
    "#print(type(tfidf))\n",
    "#print(len(tfidf.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328932, 141104)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(328932,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#convert str to float\n",
    "train_y=[]\n",
    "\n",
    "for item in y:\n",
    "    train_y.append(float(item))\n",
    "#print(type(train_y))\n",
    "print(tfidf_x.shape)\n",
    "#print(len(train_y))\n",
    "#print(tfidf_x.shape)\n",
    "print(type(tfidf_x))\n",
    "\n",
    "#train_x_formal=np.array(tfidf_x)\n",
    "#train_x_formal=tfidf_x.copy\n",
    "train_y_formal=np.array(train_y)\n",
    "#test_x=np.array(tfidf_predict_x)\n",
    "\n",
    "#print(x_formal)\n",
    "#print(train_x_formal.shape)\n",
    "#print(type(train_x_formal))\n",
    "\n",
    "print(train_y_formal.shape)\n",
    "print(type(train_y_formal))\n",
    "#start train\n",
    "model=LinearSVC()\n",
    "model.fit(tfidf_x,train_y_formal)\n",
    "#start predict\n",
    "#res=model.predict(test_x)\n",
    "\n",
    "#print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35437, 141104)\n"
     ]
    }
   ],
   "source": [
    "#start predict\n",
    "print(tfidf_predict_x.shape)\n",
    "res=model.predict(tfidf_predict_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "counter=1\n",
    "with open(\"predict.csv\",\"w\",newline=\"\") as csvfile: \n",
    "    writer = csv.writer(csvfile)\n",
    "    #先写入columns_name\n",
    "    writer.writerow([\"Id\",\"Predicted\"])\n",
    "    \n",
    "    #unsure about the type about the pred\n",
    "    for item in res:\n",
    "        writer.writerow([counter,item])\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordtag\n"
     ]
    }
   ],
   "source": [
    "str='#word'\n",
    "if str[0]=='#':\n",
    "    str=str[1:]+'tag'\n",
    "print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier#KNN knn=KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
